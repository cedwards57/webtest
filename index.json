[{"categories":["Team"],"contents":"","date":"July 1, 2024","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/armina-fani/","summary":"","tags":null,"title":"Armina Fani"},{"categories":["Team"],"contents":"","date":"January 1, 2023","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/joanne-wardell/","summary":"","tags":null,"title":"Joanne Wardell"},{"categories":["Team"],"contents":"I am a third year Ph.D. student in Electrical \u0026amp; Computer Engineering at Georgia Institute of Technology under the supervision of Dr. Sergey Plis and Dr. Vince D. Calhoun. I have interned at Microsoft Research Redmond and Mila - Quebec AI Institute.\nResearch Summary I am interested in the following topics (with selected works):\nSelf-Supervision: Natural Images [ICLR 2019], Neuroimaging [IEEE BHI 2019][MICCAI 2020][NeurIPS ML4H Workshop 2019] Multimodal Learning: Neuroimaging [IEEE ICHI 2021] Out-of-distribution Generalization, Robustness and Fairness: Neuroimaging [ICLR RobustML Workshop 2021] Interpretability: Neuroimaging [IEEE ISBI 2021], [Preprint] Image Segmentation: Brain Tissue [IJCNN 2017], Atlas [BigNeuro Workshop NeurIPS 2017] I also worked on Reinforcement Learning [NeurIPS Deep RL Workshop 2015] and Computer Vision problems (traffic sign recognition; age and gender recognition from human faces).\nI am a Ph.D. student in Electrical \u0026amp; Computer Engineering at Georgia Institute of Technology under the supervision of Dr. Sergey Plis and Dr. Vince D. Calhoun. My research interests are Representation Learning, Self-Supervision, and Multimodal Learning.\n","date":"November 11, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/alex-fedorov/","summary":"I am a third year Ph.D. student in Electrical \u0026amp; Computer Engineering at Georgia Institute of Technology under the supervision of Dr. Sergey Plis and Dr. Vince D. Calhoun. I have interned at Microsoft Research Redmond and Mila - Quebec AI Institute.\nResearch Summary I am interested in the following topics (with selected works):\nSelf-Supervision: Natural Images [ICLR 2019], Neuroimaging [IEEE BHI 2019][MICCAI 2020][NeurIPS ML4H Workshop 2019] Multimodal Learning: Neuroimaging [IEEE ICHI 2021] Out-of-distribution Generalization, Robustness and Fairness: Neuroimaging [ICLR RobustML Workshop 2021] Interpretability: Neuroimaging [IEEE ISBI 2021], [Preprint] Image Segmentation: Brain Tissue [IJCNN 2017], Atlas [BigNeuro Workshop NeurIPS 2017] I also worked on Reinforcement Learning [NeurIPS Deep RL Workshop 2015] and Computer Vision problems (traffic sign recognition; age and gender recognition from human faces).","tags":null,"title":"Alex Fedorov"},{"categories":["Team"],"contents":"Hi everyone! I got bachelor\u0026rsquo;s and master\u0026rsquo;s degrees in physics and mathematics at the Moscow Institute of Physics and Technology, after which I decided to aspire to a Ph.D. in Computer science and Machine Learning at Georgia State University.\nMy current research interests lie within the applications of Machine Learning to Neuroscience problems, mainly brain connectivity.\n","date":"January 1, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/pavel-popov/","summary":"Hi everyone! I got bachelor\u0026rsquo;s and master\u0026rsquo;s degrees in physics and mathematics at the Moscow Institute of Physics and Technology, after which I decided to aspire to a Ph.D. in Computer science and Machine Learning at Georgia State University.\nMy current research interests lie within the applications of Machine Learning to Neuroscience problems, mainly brain connectivity.","tags":null,"title":"Pavel Popov"},{"categories":["Team"],"contents":"William Stewart Ashbee is a doctoral student at Department of Computer Science at Georgia State University since Fall 2019. He is researching deep learning based brain surface reconstruction methods that produce surface meshes from MRI voxel images. At present, he has reviewed multiple methods and is preparing to combine and extend these methods for publication. This exciting area of research utilizes geometric deep learning and pytorch3d. He hopes to build a deep learning method to compete with Freesurfer. His advisor is Sergey Plis.\n","date":"August 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/william-ashbee/","summary":"William Stewart Ashbee is a doctoral student at Department of Computer Science at Georgia State University since Fall 2019. He is researching deep learning based brain surface reconstruction methods that produce surface meshes from MRI voxel images. At present, he has reviewed multiple methods and is preparing to combine and extend these methods for publication. This exciting area of research utilizes geometric deep learning and pytorch3d. He hopes to build a deep learning method to compete with Freesurfer.","tags":null,"title":"William Ashbee"},{"categories":["Team"],"contents":"I am a Ph.D. student at the Georgia Institute of Technology in the department of Electrical and Computer Engineering. I am primarily interested in Machine Learning and Deep Learning, focusing in the areas of sparsity in deep learning, efficient ML and optimization. Currently, I am working on developing novel methods of inducing sparsity in deep neural networks and studying the effects it has in the models. More recently, I have also started working in the areas of sparse reinforcement learning and multi-task learning.\n","date":"August 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/riyasat-ohib/","summary":"I am a Ph.D. student at the Georgia Institute of Technology in the department of Electrical and Computer Engineering. I am primarily interested in Machine Learning and Deep Learning, focusing in the areas of sparsity in deep learning, efficient ML and optimization. Currently, I am working on developing novel methods of inducing sparsity in deep neural networks and studying the effects it has in the models. More recently, I have also started working in the areas of sparse reinforcement learning and multi-task learning.","tags":null,"title":"Riyasat Ohib"},{"categories":["Team"],"contents":"My current research focus is causal inference from time series. I test my methods on fMRI data, which is a form of time series data. I pioneering a new method for causal structure discovery in the presence of a mismatch between the causal time scale and measurement time scale. Using graph theory I tackled limited time point time series. I\u0026rsquo;ve achieved a remarkable three orders of magnitude improvement in runtime. Furthermore, I am Investigating the use of slower measurement rates to improve causal mechanism estimation and developing an algorithm that leverages data collected at various sampling rates.\n","date":"August 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/sajad-abavisani/","summary":"My current research focus is causal inference from time series. I test my methods on fMRI data, which is a form of time series data. I pioneering a new method for causal structure discovery in the presence of a mismatch between the causal time scale and measurement time scale. Using graph theory I tackled limited time point time series. I\u0026rsquo;ve achieved a remarkable three orders of magnitude improvement in runtime. Furthermore, I am Investigating the use of slower measurement rates to improve causal mechanism estimation and developing an algorithm that leverages data collected at various sampling rates.","tags":null,"title":"Sajad Abavisani"},{"categories":["Team"],"contents":"I am pursuing a Ph.D. with a concentration in machine learning, working on NeuroImages {fMRIs} to classify and predict brain disorders and estimate disorder-specific dynamic and directed graphs of the human brain. I am interested in creating \u0026lsquo;glass-box\u0026rsquo; deep learning models that produce interpretable results.\n","date":"August 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/usman-mahmood/","summary":"I am pursuing a Ph.D. with a concentration in machine learning, working on NeuroImages {fMRIs} to classify and predict brain disorders and estimate disorder-specific dynamic and directed graphs of the human brain. I am interested in creating \u0026lsquo;glass-box\u0026rsquo; deep learning models that produce interpretable results.","tags":null,"title":"Usman Mahmood"},{"categories":["Team"],"contents":"I am interested in machine learning model optimization, multimodal data analysis, imaging inpainting, and many other machine learning algorithms. I am currently working on ICA, applying entropy maximization as the objective of the training process. I am very happy to join any machine learning related research.\n","date":"August 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/yaorong-xiao/","summary":"I am interested in machine learning model optimization, multimodal data analysis, imaging inpainting, and many other machine learning algorithms. I am currently working on ICA, applying entropy maximization as the objective of the training process. I am very happy to join any machine learning related research.","tags":null,"title":"Yaorong Xiao"},{"categories":["Team"],"contents":"Dr. Masoud is currently a postdoctoral researcher at Trends center, he received his PhD in computer science from Georgia State University. His research interest centers around the intersection region between 2D/3D/4D/multichannel Image Processing, Data Science (DS) and Web technologies to develop next generation web-based cutting-edge computational image analysis methods and tools.\n","date":"July 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/postdoc/mohamed-masoud/","summary":"Dr. Masoud is currently a postdoctoral researcher at Trends center, he received his PhD in computer science from Georgia State University. His research interest centers around the intersection region between 2D/3D/4D/multichannel Image Processing, Data Science (DS) and Web technologies to develop next generation web-based cutting-edge computational image analysis methods and tools.","tags":null,"title":"Mohamed Masoud"},{"categories":["Team"],"contents":"Minoo Jafarlou is a Ph.D. student in Computer Science at Georgia State University. She works as a graduate research assistant at TReNDS center. Her educational background is in computer engineering (BS) and computer science (MS). Minoo is interested in multimodal neuroimaging, deep learning, and machine learning. She is currently working on Few-Shot Learning.\n","date":"June 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/minoo-jafarlou/","summary":"Minoo Jafarlou is a Ph.D. student in Computer Science at Georgia State University. She works as a graduate research assistant at TReNDS center. Her educational background is in computer engineering (BS) and computer science (MS). Minoo is interested in multimodal neuroimaging, deep learning, and machine learning. She is currently working on Few-Shot Learning.","tags":null,"title":"Minoo Jafarlou"},{"categories":["Team"],"contents":"My research is primarily focussed on applying deep learning neural networks to neuroimaging data. My current research is focussed on using pre-trained models trained on time direction of FMRI data to predict and classify different abnormalities in the brain. We believe such pertained models could help learn better the brain functions. Recurrent Neural Networks work well with time series data. The goal is to develop models by exploiting the capabilities of RNNS and observe their performance on benchmark medical datasets.\n","date":"August 7, 2019","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/zafar-iqbal/","summary":"My research is primarily focussed on applying deep learning neural networks to neuroimaging data. My current research is focussed on using pre-trained models trained on time direction of FMRI data to predict and classify different abnormalities in the brain. We believe such pertained models could help learn better the brain functions. Recurrent Neural Networks work well with time series data. The goal is to develop models by exploiting the capabilities of RNNS and observe their performance on benchmark medical datasets.","tags":null,"title":"Zafar Iqbal"},{"categories":["Team"],"contents":"I am a Ph.D. candidate in Computer Science at Georgia State University. My research interests primarily lie in the shared space of XAI, medical imaging, and computer vision. In my ongoing research, I mainly focus on developing machine learning/deep learning models and leveraging explainable AI to advance our understanding of brain disorders. I put my endeavor into creating new learning algorithms, explanation methods, metrics, and frameworks useful to build interpretable AI models and apply them to broaden our understanding in the neuroscience space.\n","date":"July 1, 2019","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/phd/md-mahfuzur-rahman/","summary":"I am a Ph.D. candidate in Computer Science at Georgia State University. My research interests primarily lie in the shared space of XAI, medical imaging, and computer vision. In my ongoing research, I mainly focus on developing machine learning/deep learning models and leveraging explainable AI to advance our understanding of brain disorders. I put my endeavor into creating new learning algorithms, explanation methods, metrics, and frameworks useful to build interpretable AI models and apply them to broaden our understanding in the neuroscience space.","tags":null,"title":"Md Mahfuzur Rahman"},{"categories":["Team"],"contents":"I\u0026rsquo;m a systems engineer and researcher with a passion for machine learning (ML). My work focuses on accelerating and streamlining the training and deployment of ML models across diverse domains and environments. Currently, I specialize in facilitating the deployment of ML models for brain imaging applications. By leveraging my expertise in systems engineering and ML infrastructure, I help researchers and organizations harness the power of artificial intelligence to advance our understanding of the brain and drive innovation in neuroscience.\n","date":"June 1, 2024","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/master/mike-doan/","summary":"I\u0026rsquo;m a systems engineer and researcher with a passion for machine learning (ML). My work focuses on accelerating and streamlining the training and deployment of ML models across diverse domains and environments. Currently, I specialize in facilitating the deployment of ML models for brain imaging applications. By leveraging my expertise in systems engineering and ML infrastructure, I help researchers and organizations harness the power of artificial intelligence to advance our understanding of the brain and drive innovation in neuroscience.","tags":null,"title":"Mike Doan"},{"categories":["Team"],"contents":"Hi everyone, my name is Alexandre Castelnau. I am a French graduate student enrolled in a dual degree program between Georgia Tech and CentraleSupélec, graduate engineering school of the Université Paris-Saclay.\n","date":"June 1, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/master/alexandre-castelnau/","summary":"Hi everyone, my name is Alexandre Castelnau. I am a French graduate student enrolled in a dual degree program between Georgia Tech and CentraleSupélec, graduate engineering school of the Université Paris-Saclay.","tags":null,"title":"Alexandre Castelnau"},{"categories":["Classes"],"contents":"Machine learning studies algorithms that build models from data for subsequent use in prediction, inference, and decision making tasks. Although an active field for the last 60 years, the current demand as well as trust in machine learning exploded as increasingly more data become available and the problems needed to be addressed become literally impossible to program directly. In this advanced course we will cover essential algorithms, concepts, and principles of machine learning. Along with the traditional exposition we will learn how these principles are currently being revisited thanks to the recent discoveries in the field.\n1. Introduction Lecture Slides Youtube Lectures Introductions 7:51 Why Machine Learning 12:00 What is Machine Learning 18:57 History of Machine Learning 17:33 Reinforcement Learning 10:32 Course Overview 19:26 The Project 20:03 2. Foundations of learning Lecture Slides Youtube Lectures Formalizing the Problem of Learning 24:19 Inductive Bias 12:03 Can We Bound the Probability of Error? 25:56 3. PAC learnability Lecture Slides Youtube Lectures Main Definitions from Lecture 2 13:52 Agnostic PAC Learning 53:35 Learning via Uniform Convergence 10:15 4. Linear algebra and Optimization (recap) 3Blue1Brown Playlist 5. Linear learning models Lecture Slides Youtube Lectures Linear Decision Boundary 34:10 Perceptron 37:10 Perceptron Extensions 14:09 Linear Classifier for Linearly non Separable Classes 8:59 6. Principal Component Analysis Lecture Slides Youtube lectures Linear Regression 39:24 Linear Algebra Micro Refresher 2:04 Spectral Theorem 25:54 Principal Component Analysis 22:29 Demonstration 17:38 7. Curse of Dimensionality Lecture Slides Youtube lectures Curse of Dimensionality 1:16:27 8. Bayesian Decision Theory Lecture Slides Youtube lectures Bayesian Decision Theory 56:47 9. Parameter estimation: MLE Lecture Slides Youtube Lectures Independence 12:07 Maximum Likelihood Estimation 50:35 MLE as KL-divergence minimization 21:41 10. Parameter estimation: MAP \u0026amp; Naïve Bayes Lecture Slides Youtube Lectures MAP Estimation 56:00 The Naïve Bayes Classifier 37:09 11. Logistic Regression Lecture Slides Youtube Lectures NB to LR 19:49 Defining Logistic Regression 27:42 Solving Logistic Regression 23:35 12. Kernel Density Estimation Lecture Slides Youtube Lectures Non-parametric Density Estimation 1:13:33 13. Support Vector Machines Lecture Slides Youtube Lectures Max Margin Classifier 35:53 Lagrange Multipliers 32:45 Dual Formulation of Linear SVM 10:34 Kernel Trick and Soft Margin 27:28 14. Matrix Factorization Lecture Slides Youtube Lectures Matrix Factorization 1:24:22 15. Stochastic Gradient Descent Lecture Slides Youtube Lectures Stochastic Gradient Descent 1:06:57 16. k-means Clustering Lecture Slides Youtube Lectures Clustering 6:05 Gaussian Mixture Models 16:34 MLE recap 4:20 Hard k-means Clustering 30:27 Soft k-means Clustering 7:18 17. Expectation Maximization Lecture Slides Youtube Lectures Do we even need EM for GMM? 14:39 A \u0026ldquo;hacky\u0026rdquo; GMM estimation 15:17 MLE via EM 38:28 18. Automatic Differentiation Lecture Slides Youtube Lectures Introduction 25:10 Forward Mode AD 26:46 A minute of Backprop 2:26 Reverse mode AD 17:26 19. Nonlinear Embedding Approaches Lecture Slides Youtube Lectures Manifold Learning 20:13 20. Model Comparison I Lecture Slides Youtube Lectures Bias Variance Trade-Off 36:52 No Free Lunch Theorem 7:29 Problems with using accuracy as performance indicator 12:39 Confusion Matrix 25:15 21. Model Comparison II Lecture Slides Youtube Lectures Cross validation and hyperopt 29:08 Expected Value Framework 22:48 Visualizing Model Performance 1 31:02 Receiver Operating Characteristics 22:34 22. Model Calibration Lecture Slides Youtube Lectures On Model Calibration 36:53 23. Convolutional Neural Networks Lecture Slides Youtube Lectures Building Blocks 39:22 Skip Connection 38:46 Fully Convolutional Networks 8:07 Semantic Segmentation with Twists 23:40 Special Convolutions 20:15 24. Word Embedding Lecture Slides Youtube Lectures Introduction 10:35 Semantic Matrix 30:26 word2vec 54:22 ","date":"April 23, 2021","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/classes/advancedml/","summary":"Machine learning studies algorithms that build models from data for subsequent use in prediction, inference, and decision making tasks. Although an active field for the last 60 years, the current demand as well as trust in machine learning exploded as increasingly more data become available and the problems needed to be addressed become literally impossible to program directly. In this advanced course we will cover essential algorithms, concepts, and principles of machine learning.","tags":null,"title":"Advanced Machine Learning"},{"categories":["Team"],"contents":"I am a Research assistant at TreNDs. I work on building novel research prototypes at the intersection of computer vision, natural language, and Reinforcement Learning.\nI was also Teaching assistant and co-tutor for Intro to Deep Learning 6851 (Spring Semester 2022)\nResearch Interests Computer Vision Natural Language Generation/Understanding Multimodal Understanding Reinforcement Learning Causal/Counterfactual modelling Time Series Forecasting Multitask Learning I am pursuing Masters in Computer Science (specializing in Machine Learning) under Dr. Sergey Plis.\nI worked as a Machine Learning Engineer for 3 years at ARM, where i was a part of Machine Learning group working in the Arm’s Machine Learning Research Lab under Parth Maj where we worked on many Deep Learning problems thath were used to train, optimize and accelerate NPU chips.\nI earned a Bachelor’s degree in Computer Science in 2018 from Manipal Institute of Technology, where I was advised by Dr. Chetna Sharma. and worked on my thesis “Resume Parser for Blockchain Profile Using Novel Deep Learning”\n","date":"August 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/master/mrinal-mathur/","summary":"I am a Research assistant at TreNDs. I work on building novel research prototypes at the intersection of computer vision, natural language, and Reinforcement Learning.\nI was also Teaching assistant and co-tutor for Intro to Deep Learning 6851 (Spring Semester 2022)\nResearch Interests Computer Vision Natural Language Generation/Understanding Multimodal Understanding Reinforcement Learning Causal/Counterfactual modelling Time Series Forecasting Multitask Learning I am pursuing Masters in Computer Science (specializing in Machine Learning) under Dr.","tags":null,"title":"Mrinal Mathur"},{"categories":["Team"],"contents":"I am an AWS Certified Solution Architect with experience as a Python developer, 4 years of experience in the industry, currently pursuing masters in computer science at Georgia State University. Highly adept at handling various responsibilities by prioritizing necessary tasks, establishing clear deadlines and finding creative solutions to eliminate obstacles.\n","date":"August 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/master/pratyush-reddy/","summary":"I am an AWS Certified Solution Architect with experience as a Python developer, 4 years of experience in the industry, currently pursuing masters in computer science at Georgia State University. Highly adept at handling various responsibilities by prioritizing necessary tasks, establishing clear deadlines and finding creative solutions to eliminate obstacles.","tags":null,"title":"Pratyush Reddy"},{"categories":["Team"],"contents":"Hi, everyone. My name is Farfalla Hu. I am a multimedia designer and I studied Motion Media and Interactive Design as my undergrad at SCAD. My master\u0026rsquo;s degree is at GSU in Computer Science. Here is my personal website farfallahu.com.\n","date":"March 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/master/farfalla-hu/","summary":"Hi, everyone. My name is Farfalla Hu. I am a multimedia designer and I studied Motion Media and Interactive Design as my undergrad at SCAD. My master\u0026rsquo;s degree is at GSU in Computer Science. Here is my personal website farfallahu.com.","tags":null,"title":"Farfalla Hu"},{"categories":["Team"],"contents":"I am primarily interested in machine learning and its intersections with complex applications and theory. My current research interests are focused in leveraging insights from optimization and neural computation to interpret and innovate on Artificial Neural Networks. I am interested in novel methods for applying deep learning to neuroimaging data, especially drawing from distributed learning for performing efficient and privacy sensitive analyses in large scale, collaborative settings. I am additionally interested in the use of information theory for training and interpreting neural networks, the application of complex network theory principles to modelling neural dynamics, and drawing inspiration from neuroscience to innovate with artificial neural networks and vice-versa.\n","date":"November 11, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/postdoc/brad-baker/","summary":"I am primarily interested in machine learning and its intersections with complex applications and theory. My current research interests are focused in leveraging insights from optimization and neural computation to interpret and innovate on Artificial Neural Networks. I am interested in novel methods for applying deep learning to neuroimaging data, especially drawing from distributed learning for performing efficient and privacy sensitive analyses in large scale, collaborative settings. I am additionally interested in the use of information theory for training and interpreting neural networks, the application of complex network theory principles to modelling neural dynamics, and drawing inspiration from neuroscience to innovate with artificial neural networks and vice-versa.","tags":null,"title":"Brad Baker"},{"categories":["Team"],"contents":"Maker of projects, fan of understandable code. Genuinely enjoys writing documentation. Assisted with machine learning work on analyzing fMRI data, and also remade this website!\n","date":"May 1, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/former/capella-edwards/","summary":"I’m enthusiastic about deep learning and how its applications can help people, especially in neuroimaging and other medical contexts. My current area of focus is on diagnostics with fMRI data.","tags":null,"title":"Capella Edwards"},{"categories":["Team"],"contents":"I have completed my PhD in Biophysics and Masters in Applied Physics and Mathematics in Moscow Institute of Physics and Technology, Russia. My research interests center on the mechanisms of the brain and consciousness. My main goal is to create a technology for deliberately changing the state of consciousness to achieve states most suitable for effective work, learning, or therapy. I believe that we need high-quality brain models to reach this goal. For my PhD, I used computational modeling based on Hopfield networks, McCulloch-Pitt’s neurons, LIF, and Hodgkin-Huxley neurons. In addition, I proposed elements of communication between the architecture of artificial networks and the mechanisms of the brain. For the last couple of years, I have devoted myself to working with experimental data. My experience:\nExperiments with EEG (dry and wet electrodes): solving cognitive tests, meditation, solving problems in mathematics, perception of visual images.\nEEG data processing: spectral analysis, functional connectivity analysis, evoked potentials, elements of machine learning, calculation of information metrics (lempel-ziv, entropy).\nCurrently I am working on “Causal modeling of brain dynamical data with the goal of eventually arriving at better computational models of how the brain works.\n","date":"April 7, 2020","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/team/postdoc/kseniya-solovyeva/","summary":"I have completed my PhD in Biophysics and Masters in Applied Physics and Mathematics in Moscow Institute of Physics and Technology, Russia. My research interests center on the mechanisms of the brain and consciousness. My main goal is to create a technology for deliberately changing the state of consciousness to achieve states most suitable for effective work, learning, or therapy. I believe that we need high-quality brain models to reach this goal.","tags":null,"title":"Kseniya Solovyeva"},{"categories":["Publications"],"contents":"Domain scientists interested in causal mechanisms are usually limited by the frequency at which they can collect the measurements of social, physical, or biological systems.\nA common and plausible assumption is that higher measurement frequencies are the only way to gain more informative data about the underlying dynamical causal structure. This assumption is a strong driver for designing new, faster instruments, but such instruments might not be feasible or even possible.\nIn this paper, we show that this assumption is incorrect: there are situations in which we can gain additional information about the causal structure by measuring more than our current instruments.\nWe present an algorithm that uses graphs at multiple measurement timescales to infer underlying causal structure, and show that inclusion of structures at slower timescales can nonetheless reduce the size of the equivalence class of possible causal structures.\nWe provide simulation data about the probability of cases in which deliberate undersampling yields a gain, as well as the size of this gain.\n","date":"February 20, 2023","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/causal-learning-through-deliberate-undersampling/","summary":"Domain scientists interested in causal mechanisms are usually limited by the frequency at which they can collect the measurements of social, physical, or biological systems.\nA common and plausible assumption is that higher measurement frequencies are the only way to gain more informative data about the underlying dynamical causal structure. This assumption is a strong driver for designing new, faster instruments, but such instruments might not be feasible or even possible.","tags":["Paper","Publications","2023"],"title":"Causal Learning through Deliberate Undersampling"},{"categories":["Publications"],"contents":"Recent neuroimaging studies that focus on predicting brain disorders via modern machine learning approaches commonly include a single modality and rely on supervised over-parameterized models.However, a single modality provides only a limited view of the highly complex brain. Critically, supervised models in clinical settings lack accurate diagnostic labels for training. Coarse labels do not capture the long-tailed spectrum of brain disorder phenotypes, which leads to a loss of generalizability of the model that makes them less useful in diagnostic settings. This work presents a novel multi-scale coordinated framework for learning multiple representations from multimodal neuroimaging data. We propose a general taxonomy of informative inductive biases to capture unique and joint information in multimodal self-supervised fusion. The taxonomy forms a family of decoder-free models with reduced computational complexity and a propensity to capture multi-scale relationships between local and global representations of the multimodal inputs. We conduct a comprehensive evaluation of the taxonomy using functional and structural magnetic resonance imaging (MRI) data across a spectrum of Alzheimer\u0026rsquo;s disease phenotypes and show that self-supervised models reveal disorder-relevant brain regions and multimodal links without access to the labels during pre-training. The proposed multimodal self-supervised learning yields representations with improved classification performance for both modalities. The concomitant rich and flexible unsupervised deep learning framework captures complex multimodal relationships and provides predictive …\n","date":"September 7, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/self-supervised-multimodal-neuroimaging-yields-predictive-representations-for-a-spectrum-of-alzheimers-phenotypes/","summary":"Recent neuroimaging studies that focus on predicting brain disorders via modern machine learning approaches commonly include a single modality and rely on supervised over-parameterized models.However, a single modality provides only a limited view of the highly complex brain. Critically, supervised models in clinical settings lack accurate diagnostic labels for training. Coarse labels do not capture the long-tailed spectrum of brain disorder phenotypes, which leads to a loss of generalizability of the model that makes them less useful in diagnostic settings.","tags":["Paper","Publications","2022"],"title":"Self-supervised multimodal neuroimaging yields predictive representations for a spectrum of Alzheimer's phenotypes"},{"categories":["Publications"],"contents":"Deep learning has been widely applied in neuroimaging, including to predicting brain-phenotype relationships from magnetic resonance imaging (MRI) volumes. MRI data usually requires extensive preprocessing before it is ready for modeling, even via deep learning, in part due to its high dimensionality and heterogeneity. A growing array of MRI preprocessing pipelines have been developed each with its own strengths and limitations. Recent studies have shown that pipeline-related variation may lead to different scientific findings, even when using the identical data. Meanwhile, the machine learning community has emphasized the importance of shifting from model-centric to data-centric approaches given that data quality plays an essential role in deep learning applications. Motivated by this idea, we first evaluate how preprocessing pipeline selection can impact the downstream performance of a supervised learning model. We next propose two pipeline-invariant representation learning methodologies, MPSL and PXL, to improve consistency in classification performance and to capture similar neural network representations between pipeline pairs. Using 2000 human subjects from the UK Biobank dataset, we demonstrate that both models present unique advantages, in particular that MPSL can be used to improve out-of-sample generalization to new pipelines, while PXL can be used to improve predictive performance consistency and representational similarity within a closed pipeline set. These results suggest that our proposed models can be applied to overcome pipeline-related biases and to improve reproducibility in neuroimaging …\n","date":"August 27, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/pipeline-invariant-representation-learning-for-neuroimaging/","summary":"Deep learning has been widely applied in neuroimaging, including to predicting brain-phenotype relationships from magnetic resonance imaging (MRI) volumes. MRI data usually requires extensive preprocessing before it is ready for modeling, even via deep learning, in part due to its high dimensionality and heterogeneity. A growing array of MRI preprocessing pipelines have been developed each with its own strengths and limitations. Recent studies have shown that pipeline-related variation may lead to different scientific findings, even when using the identical data.","tags":["Paper","Publications","2022"],"title":"Pipeline-Invariant Representation Learning for Neuroimaging"},{"categories":["Publications"],"contents":"Brain dynamics are highly complex and yet hold the key to understanding brain function and dysfunction. The dynamics captured by resting-state functional magnetic resonance imaging data are noisy, high-dimensional, and not readily interpretable. The typical approach of reducing this data to low-dimensional features and focusing on the most predictive features comes with strong assumptions and can miss essential aspects of the underlying dynamics. In contrast, introspection of discriminatively trained deep learning models may uncover disorder-relevant elements of the signal at the level of individual time points and spatial locations. Yet, the difficulty of reliable training on high-dimensional low sample size datasets and the unclear relevance of the resulting predictive markers prevent the widespread use of deep learning in functional neuroimaging. In this work, we introduce a deep learning framework to learn …\n","date":"July 21, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/interpreting-models-interpreting-brain-dynamics/","summary":"Brain dynamics are highly complex and yet hold the key to understanding brain function and dysfunction. The dynamics captured by resting-state functional magnetic resonance imaging data are noisy, high-dimensional, and not readily interpretable. The typical approach of reducing this data to low-dimensional features and focusing on the most predictive features comes with strong assumptions and can miss essential aspects of the underlying dynamics. In contrast, introspection of discriminatively trained deep learning models may uncover disorder-relevant elements of the signal at the level of individual time points and spatial locations.","tags":["Paper","Publications","2022"],"title":"Interpreting models interpreting brain dynamics"},{"categories":["Publications"],"contents":"Mental disorders such as schizophrenia have been challenging to characterize due in part to their heterogeneous presentation in individuals. Most studies have focused on identifying groups differences and have typically ignored the heterogeneous patterns within groups. Here we propose a novel approach based on a variational autoencoder (VAE) to interpolate static functional network connectivity (sFNC) across individuals, with group-specific patterns between schizophrenia patients and controls captured simultaneously. We then visualize the original sFNC in a 2D grid according to the samples in the VAE latent space. We observe a high correspondence between the generated and the original sFNC. The proposed framework facilitates data visualization and can potentially be applied to predict the stage that a subject falls within a disorder continuum as well as characterize individual heterogeneity within and …\n","date":"July 11, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/mind-the-gap-functional-network-connectivity-interpolation-between-schizophrenia-patients-and-controls-using-a-variational-autoencoder/","summary":"Mental disorders such as schizophrenia have been challenging to characterize due in part to their heterogeneous presentation in individuals. Most studies have focused on identifying groups differences and have typically ignored the heterogeneous patterns within groups. Here we propose a novel approach based on a variational autoencoder (VAE) to interpolate static functional network connectivity (sFNC) across individuals, with group-specific patterns between schizophrenia patients and controls captured simultaneously. We then visualize the original sFNC in a 2D grid according to the samples in the VAE latent space.","tags":["Paper","Publications","2022"],"title":"Mind the gap: functional network connectivity interpolation between schizophrenia patients and controls using a variational autoencoder"},{"categories":["Publications"],"contents":"Dynamic functional network connectivity (dFNC) analysis is a widely used approach for capturing brain activation patterns, connectivity states, and network organization. However, a typical sliding window plus clustering (SWC) approach for analyzing dFNC models the system through a fixed sequence of connectivity states. SWC assumes connectivity patterns span throughout the brain, but they are relatively spatially constrained and temporally short‐lived in practice. Thus, SWC is neither designed to capture transient dynamic changes nor heterogeneity across subjects/time. We propose a state‐space time series summarization framework called “statelets” to address these shortcomings. It models functional connectivity dynamics at fine‐grained timescales, adapting time series motifs to changes in connectivity strength, and constructs a concise yet informative representation of the original data that conveys easily …\n","date":"June 1, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/statelets-capturing-recurrent-transient-variations-in-dynamic-functional-network-connectivity/","summary":"Dynamic functional network connectivity (dFNC) analysis is a widely used approach for capturing brain activation patterns, connectivity states, and network organization. However, a typical sliding window plus clustering (SWC) approach for analyzing dFNC models the system through a fixed sequence of connectivity states. SWC assumes connectivity patterns span throughout the brain, but they are relatively spatially constrained and temporally short‐lived in practice. Thus, SWC is neither designed to capture transient dynamic changes nor heterogeneity across subjects/time.","tags":["Paper","Publications","2022"],"title":"Statelets: Capturing recurrent transient variations in dynamic functional network connectivity"},{"categories":["Publications"],"contents":"Graphical structures estimated by causal learning algorithms from time series data can provide highly misleading causal information if the causal timescale of the generating process fails to match the measurement timescale of the data. Although this problem has been recently recognized, practitioners have limited resources to respond to it, and so must continue using models that they know are likely misleading. Existing methods either (a) require that the difference between causal and measurement timescales is known; or (b) can handle only very small number of random variables when the timescale difference is unknown; or (c) apply to only pairs of variables, though with fewer assumptions about prior knowledge; or (d) return impractically too many solutions. This paper addresses all four challenges. We combine constraint programming with both theoretical insights into the problem structure and prior information about admissible causal interactions. The resulting system provides a practical approach that scales to significantly larger sets (\u0026gt;100) of random variables, does not require precise knowledge of the timescale difference, supports edge misidentification and parametric connection strengths, and can provide the optimum choice among many possible solutions. The cumulative impact of these improvements is gain of multiple orders of magnitude in speed and informativeness.\n","date":"May 18, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/constraint-based-causal-structure-learning-from-undersampled-graphs/","summary":"Graphical structures estimated by causal learning algorithms from time series data can provide highly misleading causal information if the causal timescale of the generating process fails to match the measurement timescale of the data. Although this problem has been recently recognized, practitioners have limited resources to respond to it, and so must continue using models that they know are likely misleading. Existing methods either (a) require that the difference between causal and measurement timescales is known; or (b) can handle only very small number of random variables when the timescale difference is unknown; or (c) apply to only pairs of variables, though with fewer assumptions about prior knowledge; or (d) return impractically too many solutions.","tags":["Paper","Publications","2022"],"title":"Constraint-Based Causal Structure Learning from Undersampled Graphs"},{"categories":["Publications"],"contents":"Privacy concerns for rare disease data, institutional or IRB policies, access to local computational or storage resources or download capabilities are among the reasons that may preclude analyses that pool data to a single site. A growing number of multisite projects and consortia were formed to function in the federated environment to conduct productive research under constraints of this kind. In this scenario, a quality control tool that visualizes decentralized data in its entirety via global aggregation of local computations is especially important, as it would allow the screening of samples that cannot be jointly evaluated otherwise. To solve this issue, we present two algorithms: decentralized data stochastic neighbor embedding, dSNE, and its differentially private counterpart, DP‐dSNE. We leverage publicly available datasets to simultaneously map data samples located at different sites according to their similarities …\n","date":"May 1, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/privacypreserving-quality-control-of-neuroimaging-datasets-in-federated-environments/","summary":"Privacy concerns for rare disease data, institutional or IRB policies, access to local computational or storage resources or download capabilities are among the reasons that may preclude analyses that pool data to a single site. A growing number of multisite projects and consortia were formed to function in the federated environment to conduct productive research under constraints of this kind. In this scenario, a quality control tool that visualizes decentralized data in its entirety via global aggregation of local computations is especially important, as it would allow the screening of samples that cannot be jointly evaluated otherwise.","tags":["Paper","Publications","2022"],"title":"Privacy‐preserving quality control of neuroimaging datasets in federated environments"},{"categories":["Publications"],"contents":"Recent studies have demonstrated that neuroimaging data can be used to estimate biological brain age, as it captures information about the neuroanatomical and functional changes the brain undergoes during development and the aging process. However, researchers often have limited access to neuroimaging data because of its challenging and expensive acquisition process, thereby limiting the effectiveness of the predictive model. Decentralized models provide a way to build more accurate and generalizable prediction models, bypassing the traditional data-sharing methodology. In this work, we propose a decentralized method for biological brain age estimation using support vector regression models and evaluate it on three different feature sets, including both volumetric and voxelwise structural MRI data as well as resting functional MRI data. The results demonstrate that our decentralized brain age …\n","date":"April 5, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/decentralized-brain-age-estimation-using-mri-data/","summary":"Recent studies have demonstrated that neuroimaging data can be used to estimate biological brain age, as it captures information about the neuroanatomical and functional changes the brain undergoes during development and the aging process. However, researchers often have limited access to neuroimaging data because of its challenging and expensive acquisition process, thereby limiting the effectiveness of the predictive model. Decentralized models provide a way to build more accurate and generalizable prediction models, bypassing the traditional data-sharing methodology.","tags":["Paper","Publications","2022"],"title":"Decentralized Brain Age Estimation using MRI Data"},{"categories":["Publications"],"contents":"Advances in imaging acquisition techniques allow multiple imaging modalities to be collected from the same subject. Each individual modality offers limited yet unique views of the functional, structural, or dynamic temporal features of the brain. Multimodal fusion provides effective ways to leverage these complementary perspectives from multiple modalities. However, the majority of current multimodal fusion approaches involving functional magnetic resonance imaging (fMRI) are limited to 3D feature summaries that do not incorporate its rich temporal information. Thus, we propose a novel three‐way parallel group independent component analysis (pGICA) fusion method that incorporates the first‐level 4D fMRI data (temporal information included) by parallelizing group ICA into parallel ICA via a unified optimization framework. A new variability matrix was defined to capture subject‐wise functional variability and then …\n","date":"March 1, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/three-way-parallel-group-independent-component-analysis-fusion-of-spatial-and-spatiotemporal-magnetic-resonance-imaging-data/","summary":"Advances in imaging acquisition techniques allow multiple imaging modalities to be collected from the same subject. Each individual modality offers limited yet unique views of the functional, structural, or dynamic temporal features of the brain. Multimodal fusion provides effective ways to leverage these complementary perspectives from multiple modalities. However, the majority of current multimodal fusion approaches involving functional magnetic resonance imaging (fMRI) are limited to 3D feature summaries that do not incorporate its rich temporal information.","tags":["Paper","Publications","2022"],"title":"Three‐way parallel group independent component analysis: Fusion of spatial and spatiotemporal magnetic resonance imaging data"},{"categories":["Publications"],"contents":"Deep learning (DL) has been extremely successful when applied to the analysis of natural images. By contrast, analyzing neuroimaging data presents some unique challenges, including higher dimensionality, smaller sample sizes, multiple heterogeneous modalities, and a limited ground truth. In this article, we discuss DL methods in the context of four diverse and important categories in the neuroimaging field: classification/prediction, dynamic activity/connectivity, multimodal fusion, and interpretation/visualization. We highlight recent progress in each of these categories, discuss the benefits of combining data characteristics and model architectures, and derive guidelines for the use of DL in neuroimaging data. For each category, we also assess promising applications and major challenges to overcome. Finally, we discuss future directions of neuroimaging DL for clinical applications, a topic of great interest\u0026hellip;\n","date":"February 24, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/deep-learning-in-neuroimaging-promises-and-challenges/","summary":"Deep learning (DL) has been extremely successful when applied to the analysis of natural images. By contrast, analyzing neuroimaging data presents some unique challenges, including higher dimensionality, smaller sample sizes, multiple heterogeneous modalities, and a limited ground truth. In this article, we discuss DL methods in the context of four diverse and important categories in the neuroimaging field: classification/prediction, dynamic activity/connectivity, multimodal fusion, and interpretation/visualization. We highlight recent progress in each of these categories, discuss the benefits of combining data characteristics and model architectures, and derive guidelines for the use of DL in neuroimaging data.","tags":["Paper","Publications","2022"],"title":"Deep learning in neuroimaging: Promises and challenges"},{"categories":["Classes"],"contents":"Introduction to Deep Learning Welcome to the introduction to deep learning course! ​ This course is designed to provide you with a solid foundation in the fundamentals of deep learning. Throughout this course, you will learn about the basic building blocks of deep learning, including basics of machine learning, convolutional neural networks, and natural language processing. You will also gain an understanding of how deep learning algorithms are used to solve a variety of real-world problems, such as image classification, natural language processing and a few advance approaches such as GANs. ​\nBy the end of the course, you will have a solid understanding of the core concepts and techniques used in deep learning, as well as hands-on experience building and training your own deep learning models using popular frameworks such as PyTorch and Catalyst ​\nDr. Sergey Plis is the instructor for this course, bringing his expertise of an active researcher in the fields of neuroscience and computer science. He has extensive experience applying machine learning algorithms to the analysis of brain imaging data. He is also an experienced educator, having taught numerous courses in data science, machine learning, and deep learning at the graduate and undergraduate levels. ​\nThe hands-on part of the course has been developed by Mrinal Mathur, a seasoned machine learning engineer with experience building and deploying machine learning models for a variety of industries. Mrinal has a deep understanding of the underlying mathematical and statistical concepts that power deep learning algorithms, and he has a passion for teaching others about the exciting possibilities of this field. ​\nTogether, we have designed a comprehensive and engaging course that will provide you with the knowledge and skills you need to succeed in the exciting field of deep learning. ​\nIntroduction to Deep Learning 1. Introduction Lecture Slides Introduction to Collab Pandas (optional) Lecture Slides Numpy Machine Learning 2. Foundations of Machine Learning Calculus and Optimization\nLecture Slides Linear Regression/Classification\nLecture Slides Perceptron\nLecture Slides 3. Automatic Differentitation Lecture Slides Colab Notebooks Automatic Differentitation programming 4. Practice for Automatic Differentiation Lecture Slides 5. Pytorch Colab Notebooks\nIntroduction to Pytorch and Catalyst 6. Model Comparision Lecture slides Colab Notebook Model Comparision Computer Vision 7. Computer Vision and Image Processing Lecture Slides Colab Notebook\nIntroduction to Computer Vision 8. Convolution Neural Network Lecture Slides Colab Notebook:\nIntroduction to CNNs 9. Image Classification Lecture Slides Colab Notebook\nClassification in Computer vision 10. Skip Connections and ResNets Colab Notebook\nIntro to Skip Connections and ResNets 11. Segmentation Lecture Slides Colab Notebook\nImage Segmentation 12. Auto-Encoders Lecture Slides Colab Notebooks:\nAuto-Encoders Variational Autoencoders 13. Generative Adversarial Nets Lecture Slides Colab Notebook:\nGenerative Adversarial Nets 14. Regularization Lecture Slides Natural Language Processing 15. Introduction to NLP Lecture Slides Colab Notebooks:\nIntroduction to NLP 16. Recurrent Neural Networks Colab Notebooks:\nRNNs 17. LSTM and GRU Lecture Slides Colab Notebook:\nLSTM and GRU in pytorch 18. Seq2Seq2 Lecture Slides Colab Notebooks:\nseq2seq models 19. Attention is all you need! Lecture Slides Colab Notebooks:\nAttention Mechanisms 20. Transformers Lecture Slides Colab Notebooks:\nTransformers Advance Topics (To be added) [Graph Neural Networks] [Reinforcement Learning] [Meta Learning] [Adversarial Learning] [Transfer Learning] [Self Supervised Learning] [Few Shot Learning] [Active Learning] [Multi Task Learning] [Multi Modal Learning] [Domain Adaptation] [Continual Learning] [Causal Learning] ","date":"February 14, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/classes/introduction_to_dl/","summary":"Introduction to Deep Learning Welcome to the introduction to deep learning course! ​ This course is designed to provide you with a solid foundation in the fundamentals of deep learning. Throughout this course, you will learn about the basic building blocks of deep learning, including basics of machine learning, convolutional neural networks, and natural language processing. You will also gain an understanding of how deep learning algorithms are used to solve a variety of real-world problems, such as image classification, natural language processing and a few advance approaches such as GANs.","tags":null,"title":"Introduction to Deep Learning"},{"categories":["Publications"],"contents":"With the growth of decentralized/federated analysis approaches in neuroimaging, the opportunities to study brain disorders using data from multiple sites has grown multi-fold. One such initiative is the Neuromark, a fully automated spatially constrained independent component analysis (ICA) that is used to link brain network abnormalities among different datasets, studies, and disorders while leveraging subject-specific networks. In this study, we implement the neuromark pipeline in COINSTAC, an open-source neuroimaging framework for collaborative/decentralized analysis. Decentralized analysis of nearly 2000 resting-state functional magnetic resonance imaging datasets collected at different sites across two cohorts and co-located in different countries was performed to study the resting brain functional network connectivity changes in adolescents who smoke and consume alcohol. Results showed hypoconnectivity across the majority of networks including sensory, default mode, and subcortical domains, more for alcohol than smoking, and decreased low frequency power. These findings suggest that global reduced synchronization is associated with both tobacco and alcohol use. This work demonstrates the utility and incentives associated with large-scale decentralized collaborations spanning multiple sites.\n","date":"January 1, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/federated-analysis-in-coinstac-reveals-functional-network-connectivity-and-spectral-links-to-smoking-and-alcohol-consumption-in-nearly-2000-adolescent-brains/","summary":"With the growth of decentralized/federated analysis approaches in neuroimaging, the opportunities to study brain disorders using data from multiple sites has grown multi-fold. One such initiative is the Neuromark, a fully automated spatially constrained independent component analysis (ICA) that is used to link brain network abnormalities among different datasets, studies, and disorders while leveraging subject-specific networks. In this study, we implement the neuromark pipeline in COINSTAC, an open-source neuroimaging framework for collaborative/decentralized analysis.","tags":["Paper","Publications","2022"],"title":"Federated analysis in COINSTAC reveals functional network connectivity and spectral links to smoking and alcohol consumption in nearly 2,000 adolescent brains"},{"categories":["Publications"],"contents":"Graph-theoretical methods have been widely used to study human brain networks in psychiatric disorders. However, the focus has primarily been on global graphic metrics with little attention to the information contained in paths connecting brain regions. Details of disruption of these paths may be highly informative for understanding disease mechanisms. To detect the absence or addition of multistep paths in the patient group, we provide an algorithm estimating edges that contribute to these paths with reference to the control group. We next examine where pairs of nodes were connected through paths in both groups by using a covariance decomposition method. We apply our method to study resting-state fMRI data in schizophrenia versus controls. Results show several disconnectors in schizophrenia within and between functional domains, particularly within the default mode and cognitive control networks …\n","date":"January 1, 2022","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/path-analysis-a-method-to-estimate-altered-pathways-in-time-varying-graphs-of-neuroimaging-data/","summary":"Graph-theoretical methods have been widely used to study human brain networks in psychiatric disorders. However, the focus has primarily been on global graphic metrics with little attention to the information contained in paths connecting brain regions. Details of disruption of these paths may be highly informative for understanding disease mechanisms. To detect the absence or addition of multistep paths in the patient group, we provide an algorithm estimating edges that contribute to these paths with reference to the control group.","tags":["Paper","Publications","2022"],"title":"Path analysis: A method to estimate altered pathways in time-varying graphs of neuroimaging data"},{"categories":["Publications"],"contents":"Deep Reinforcement Learning (RL) is a powerful framework for solving complex real-world problems. Large neural networks employed in the framework are traditionally associated with better generalization capabilities, but their increased size entails the drawbacks of extensive training duration, substantial hardware resources, and longer inference times. One way to tackle this problem is to prune neural networks leaving only the necessary parameters. State-of-the-art concurrent pruning techniques for imposing sparsity perform demonstrably well in applications where data distributions are fixed. However, they have not yet been substantially explored in the context of RL. We close the gap between RL and single-shot pruning techniques and present a general pruning approach to the Offline RL. We leverage a fixed dataset to prune neural networks before the start of RL training. We then run experiments varying the network sparsity level and evaluating the validity of pruning at initialization techniques in continuous control tasks. Our results show that with 95% of the network weights pruned, Offline-RL algorithms can still retain performance in the majority of our experiments. To the best of our knowledge, no prior work utilizing pruning in RL retained performance at such high levels of sparsity. Moreover, pruning at initialization techniques can be easily integrated into any existing Offline-RL algorithms without changing the learning objective.\n","date":"December 31, 2021","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/single-shot-pruning-for-offline-reinforcement-learning/","summary":"Deep Reinforcement Learning (RL) is a powerful framework for solving complex real-world problems. Large neural networks employed in the framework are traditionally associated with better generalization capabilities, but their increased size entails the drawbacks of extensive training duration, substantial hardware resources, and longer inference times. One way to tackle this problem is to prune neural networks leaving only the necessary parameters. State-of-the-art concurrent pruning techniques for imposing sparsity perform demonstrably well in applications where data distributions are fixed.","tags":["Paper","Publications","2021"],"title":"Single-shot pruning for offline reinforcement learning"},{"categories":["Publications"],"contents":"The field of neuroimaging has embraced sharing data to collaboratively advance our understanding of the brain. However, data sharing, especially across sites with large amounts of protected health information (PHI), can be cumbersome and time intensive. Recently, there has been a greater push towards collaborative frameworks that enable large-scale federated analysis of neuroimaging data without the data having to leave its original location. However, there still remains a need for a standardized federated approach that not only allows for data sharing adhering to the FAIR (Findability, Accessibility, Interoperability, Reusability) data principles, but also streamlines analyses and communication while maintaining subject privacy. In this paper, we review a non-exhaustive list of neuroimaging analytic tools and frameworks currently in use. We then provide an update on our federated neuroimaging analysis …\n","date":"November 22, 2021","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/federated-analysis-of-neuroimaging-data-a-review-of-the-field/","summary":"The field of neuroimaging has embraced sharing data to collaboratively advance our understanding of the brain. However, data sharing, especially across sites with large amounts of protected health information (PHI), can be cumbersome and time intensive. Recently, there has been a greater push towards collaborative frameworks that enable large-scale federated analysis of neuroimaging data without the data having to leave its original location. However, there still remains a need for a standardized federated approach that not only allows for data sharing adhering to the FAIR (Findability, Accessibility, Interoperability, Reusability) data principles, but also streamlines analyses and communication while maintaining subject privacy.","tags":["Paper","Publications","2021"],"title":"Federated analysis of neuroimaging data: A review of the field"},{"categories":["Publications"],"contents":"Interpretability methods for deep neural networks mainly focus on modifying the rules of automatic differentiation or perturbing the input and observing the score drop to determine the most relevant features. Among them, gradient-based attribution methods, such as saliency maps, are arguably the most popular. Still, the produced saliency maps may often lack intelligibility. We address this problem based on recent discoveries in geometric properties of deep neural networks\u0026rsquo; loss landscape that reveal the existence of a multiplicity of local minima in the vicinity of a trained model\u0026rsquo;s loss surface. We introduce two methods that leverage the geometry of the loss landscape to improve interpretability: 1)\u0026quot; Geometrically Guided Integrated Gradients\u0026quot;, applying gradient ascent to each interpolation point of the linear path as a guide. 2)\u0026quot; Geometric Ensemble Gradients\u0026quot;, generating ensemble saliency maps by sampling proximal iso-loss models. Compared to vanilla and integrated gradients, these methods significantly improve saliency maps in quantitative and visual terms. We verify our findings on MNIST and Imagenet datasets across convolutional, ResNet, and Inception V3 architectures.\n","date":"March 1, 2014","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/geometrically-guided-saliency-maps/","summary":"Interpretability methods for deep neural networks mainly focus on modifying the rules of automatic differentiation or perturbing the input and observing the score drop to determine the most relevant features. Among them, gradient-based attribution methods, such as saliency maps, are arguably the most popular. Still, the produced saliency maps may often lack intelligibility. We address this problem based on recent discoveries in geometric properties of deep neural networks\u0026rsquo; loss landscape that reveal the existence of a multiplicity of local minima in the vicinity of a trained model\u0026rsquo;s loss surface.","tags":["Paper","Publications","2014"],"title":"Geometrically Guided Saliency Maps"},{"categories":["Publications"],"contents":"Spontaneous fluctuations are a hallmark of recordings of neural signals, emergent over time scales spanning milliseconds and tens of minutes. However, investigations of intrinsic brain organization based on resting-state functional magnetic resonance imaging have largely not taken into account the presence and potential of temporal variability, as most current approaches to examine functional connectivity (FC) implicitly assume that relationships are constant throughout the length of the recording. In this work, we describe an approach to assess whole-brain FC dynamics based on spatial independent component analysis, sliding time window correlation, and k-means clustering of windowed correlation matrices. The method is applied to resting-state data from a large sample (n = 405) of young adults. Our analysis of FC variability highlights particularly flexible connections between regions in lateral parietal …\n","date":"March 1, 2014","hero":"/webtest/images/default-hero.png","permalink":"https://cedwards57.github.io/webtest/publications/tracking-whole-brain-connectivity-dynamics-in-the-resting-state/","summary":"Spontaneous fluctuations are a hallmark of recordings of neural signals, emergent over time scales spanning milliseconds and tens of minutes. However, investigations of intrinsic brain organization based on resting-state functional magnetic resonance imaging have largely not taken into account the presence and potential of temporal variability, as most current approaches to examine functional connectivity (FC) implicitly assume that relationships are constant throughout the length of the recording. In this work, we describe an approach to assess whole-brain FC dynamics based on spatial independent component analysis, sliding time window correlation, and k-means clustering of windowed correlation matrices.","tags":["Paper","Publications","2014"],"title":"Tracking whole-brain connectivity dynamics in the resting state"}]